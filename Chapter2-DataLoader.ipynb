{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7a8cafa8-fd9f-41cd-89b7-eaff76fa5246",
   "metadata": {},
   "source": [
    "# Chapter 2 에서 배울 내용\n",
    "\n",
    "1. **분자 데이터 수집**  \n",
    "   - 공개 데이터셋 다운로드 및 데이터 구조 확인\n",
    "     \n",
    "2. **분자 데이터 변환**  \n",
    "   - SMILES 코드를 활용하여 Task에 적합한 형태로 데이터 변환  \n",
    "     \n",
    "3. **데이터 로더 제작**  \n",
    "   - 모델 학습을 위한 데이터 로더 설계 및 구현\n",
    "   - 분자 데이터 분류를 위한 사전 지식 이해\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6294bcb0-c7ff-4906-aa97-0d63d96e7ab3",
   "metadata": {},
   "source": [
    "### Ignore warnings\n",
    "코드 실행에서 불필요한 경고 문구 해제 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9e156edc-63cf-4162-bf7d-ebf06f974434",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "from rdkit import RDLogger\n",
    "\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "RDLogger.DisableLog('rdApp.*')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4687ef5a-7bbe-4f05-aa2e-660ac356a99c",
   "metadata": {},
   "source": [
    "### Library Import\n",
    "코드 실행에 필요한 라이브러리를 호출"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f4f9afab-6502-4d12-8669-1f378cebe631",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024.03.6\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import rdkit\n",
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from rdkit import Chem\n",
    "from deepchem.feat.smiles_tokenizer import BasicSmilesTokenizer\n",
    "from rdkit.Chem import AllChem, Descriptors, rdDepictor, rdDistGeom, MACCSkeys, rdMolDescriptors\n",
    "\n",
    "from torch.utils.data import Dataset\n",
    "from torch_geometric import utils as pyg_utils\n",
    "from torch_geometric.data import InMemoryDataset, download_url, extract_gz, Data, DataLoader, Batch\n",
    "\n",
    "# 작업을 위한 별도의 함수 불러오기\n",
    "from utils.splitters import random_split, scaffold_split ## split\n",
    "from utils.download_preprocess import CustomMoleculeNet, atom_features, EDGE_FEATURES\n",
    "\n",
    "print(rdkit.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e67c065-654f-454f-8ba5-e4b30e7d6da7",
   "metadata": {},
   "source": [
    "## 1) 분자 데이터 수집\n",
    "\n",
    "**MoleculeNet**은 화학 및 생물학적 데이터 분석과 모델링을 위한 벤치마크 데이터셋 모음이다.\n",
    "\n",
    "주로 분자 특성 예측 및 약물 설계와 같은 분야에서 활용되며, 다양한 분자 특성과 데이터 분포를 포함하고 있다.\n",
    "\n",
    "torch_geometric은 MoleculeNet 데이터를 간단하게 다운로드하고 처리할 수 있도록 지원한다.\n",
    "\n",
    "이를 편리하게 이용할 수 있도록 본 예제 코드에서는 별도로 제작한 **CustomMoleculeNet** 함수를 이용한다.\n",
    "\n",
    "MoleculeNet에서 제공하는 데이터셋은 목적에 따라 **Classification**과 **Regression**으로 분류할 수 있다.\n",
    "\n",
    "아래 표는 MoleculeNet에서 제공하는 주요 데이터셋에 대해 정리한 표다.\n",
    "\n",
    "---\n",
    "\n",
    "| 데이터셋 이름   | 문제 유형      | 설명                           |\n",
    "|-----------------|---------------|--------------------------------|\n",
    "| **HIV**         | Classification | 항바이러스 물질 활성 예측 (이진 분류) |\n",
    "| **BACE**        | Classification | 알츠하이머 치료 물질 활성 예측 (이진 분류) |\n",
    "| **BBBP**        | Classification | 혈액-뇌 장벽 투과성 예측 (이진 분류) |\n",
    "| **Tox21**       | Classification | 화합물 독성 예측 (이진 분류)   |\n",
    "| **ToxCast**     | Classification | 생물학적 독성 예측 (이진 분류) |\n",
    "| **SIDER**       | Classification | 부작용 예측 (다중 클래스 분류) |\n",
    "| **ClinTox**     | Classification | 독성 및 허가 여부 예측 (이진 분류) |\n",
    "| **ESOL**        | Regression     | 분자의 용해도 예측            |\n",
    "| **FreeSolv**    | Regression     | 분자의 수화 에너지 예측       |\n",
    "| **Lipo**        | Regression     | 소수성(LogD) 예측             |\n",
    "\n",
    "---\n",
    "\n",
    "이번 실습에서는 **Classification 작업**을 위한 **BACE 데이터셋**과 **Regression 작업**을 위한 **ESOL 데이터셋**을 다운로드하여 활용한다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b185c499-ed3c-4a3b-9e8e-0079b2ae6f8c",
   "metadata": {},
   "source": [
    "### Data Download\n",
    "\n",
    "CustomMoleculeNet을 이용하여 원하는 데이터셋을 다운로드 받아보자."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "65cbe79e-0a63-41dc-b2e9-ed102204e765",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data processed and saved successfully.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing...\n",
      "Done!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "BACE(1)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# dataset 다운로드 - smiles와 label을 저장\n",
    "bace = CustomMoleculeNet('dataset', name='BACE')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f76a75a-95ec-46de-af77-651192c7429e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset 다운로드 - smiles와 label을 저장\n",
    "esol = CustomMoleculeNet('dataset', name='ESOL')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04f98bca-a840-4724-af78-b3047629ccf3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "53ff281a-20ae-4ce6-a1a5-0e2ebdc15fe7",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_list = torch.load('dataset/' + dataset_name + '/processed/smiles_labels.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "efe9f605-8b8d-4cd2-8d24-c5642905001b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1513,\n",
       " 'O1CC[C@@H](NC(=O)[C@@H](Cc2cc3cc(ccc3nc2N)-c2ccccc2C)C)CC1(C)C',\n",
       " [[1.0]])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(data_list), data_list[0]['smiles'], data_list[0]['label']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8e6779b-da3b-4767-9e0f-1b2abe752ccb",
   "metadata": {},
   "source": [
    "### Graph Data로 변환"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5c0c4d2e-93f8-455c-97f4-7a4156cbc5ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# smiles를 graph data로 변환\n",
    "def smiles_to_graph(data_list: list, with_hydrogen: bool = False, kekulize: bool = False) :\n",
    "\n",
    "    graph_list = []\n",
    "    for data in data_list:\n",
    "        smiles = data['smiles']\n",
    "        label = data['label']\n",
    "        \n",
    "        mol = Chem.MolFromSmiles(smiles)\n",
    "        \n",
    "        # smiles가 객체로 변환되지 않는 경우 filtering\n",
    "        if mol is None :\n",
    "            continue\n",
    "        else: \n",
    "            # mol -> graph\n",
    "            if with_hydrogen:\n",
    "                mol = Chem.AddHs(mol)\n",
    "            if kekulize:\n",
    "                Chem.Kekulize(mol)\n",
    "        \n",
    "            xs: List[List[int]] = []\n",
    "            for atom in mol.GetAtoms():\n",
    "                current_atom_feat = atom_features(atom)\n",
    "                xs.append(current_atom_feat)\n",
    "        \n",
    "            x = torch.tensor(xs, dtype=torch.long).view(-1, 133)\n",
    "        \n",
    "            edge_indices, edge_attrs = [], []\n",
    "            for bond in mol.GetBonds():\n",
    "                i = bond.GetBeginAtomIdx()\n",
    "                j = bond.GetEndAtomIdx()\n",
    "        \n",
    "                edge_feature = [EDGE_FEATURES['possible_bonds'].index(bond.GetBondType())] + [\n",
    "                    EDGE_FEATURES['possible_bond_dirs'].index(bond.GetBondDir())]\n",
    "        \n",
    "                edge_indices += [[i, j], [j, i]]\n",
    "                edge_attrs += [edge_feature, edge_feature]\n",
    "        \n",
    "            edge_index = torch.tensor(edge_indices)\n",
    "            edge_index = edge_index.t().to(torch.long).view(2, -1)\n",
    "            edge_attr = torch.tensor(edge_attrs, dtype=torch.long).view(-1, 2)\n",
    "        \n",
    "            if edge_index.numel() > 0:  # Sort indices.\n",
    "                perm = (edge_index[0] * x.size(0) + edge_index[1]).argsort()\n",
    "                edge_index, edge_attr = edge_index[:, perm], edge_attr[perm]\n",
    "\n",
    "            # label -> y\n",
    "            y = torch.tensor([label], dtype=torch.float).view(1, -1)\n",
    "            graph_list.append(Data(x=x, edge_index=edge_index, edge_attr=edge_attr, smiles=smiles, y=y))\n",
    "    \n",
    "    return graph_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9dfbbadd-309c-4b50-b234-af220a14c0b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "graph_list = smiles_to_graph(data_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "35d8b1e4-14bf-4747-b67b-77956ca39ba9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1513, 1513)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(data_list),len(graph_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ce7856a6-4332-433f-a4af-e5a84f0a04ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 1\n",
    "train_dataset, valid_dataset, test_dataset = random_split(graph_list, null_value=0, frac_train=0.8, frac_valid=0.1, frac_test=0.1, seed=seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8bf344ef-dbeb-45e7-8942-b5710a79e2f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def loader_dataset(data_list, batch_size, shuffle=False):\n",
    "    \"\"\"\n",
    "    DataLoader로 변경\n",
    "    \"\"\"\n",
    "    collate = Batch.from_data_list(data_list)\n",
    "    loader = DataLoader(data_list, batch_size=batch_size, collate_fn=collate, shuffle=shuffle)\n",
    "\n",
    "    return loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9128401d-17ae-4c5f-bd50-c26ccefe1596",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 256\n",
    "train_loader = loader_dataset(data_list=train_dataset, batch_size=batch_size, shuffle=False)\n",
    "valid_loader = loader_dataset(data_list=valid_dataset, batch_size=batch_size, shuffle=False) \n",
    "test_loader = loader_dataset(data_list=test_dataset, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3ef0384d-9b36-41f5-80c6-a5c4a43c288a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataBatch(x=[8647, 133], edge_index=[2, 18706], edge_attr=[18706, 2], y=[256, 1], smiles=[256], batch=[8647], ptr=[257])\n"
     ]
    }
   ],
   "source": [
    "# loader 확인\n",
    "for batch in train_loader:\n",
    "    print(batch)\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba29094d-cdde-4599-97df-e3d27260d78f",
   "metadata": {},
   "source": [
    "### 문자열 token으로 변환"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "96f87735-a14b-4672-aca7-2d24b967b569",
   "metadata": {},
   "outputs": [],
   "source": [
    "def smiles_to_token(data_list):\n",
    "    # token 종류 수집\n",
    "    vocab = []\n",
    "    max_len = 0\n",
    "    tokenizer = BasicSmilesTokenizer()\n",
    "    for data in data_list:\n",
    "        tokens = tokenizer.tokenize(data['smiles'])\n",
    "        max_len = max(max_len, len(tokens))\n",
    "        vocab += tokens\n",
    "        \n",
    "    uniq_vocab = sorted(set(vocab))\n",
    "    smiles_vocab = {v: i for i, v in enumerate(uniq_vocab)}\n",
    "    smiles_vocab['Unk'] = len(smiles_vocab)\n",
    "\n",
    "    # token으로 변환\n",
    "    tokens_list = []\n",
    "    for data in data_list :\n",
    "        label = data['label']\n",
    "        tokens = [smiles_vocab[token] for token in tokenizer.tokenize(data['smiles'])]\n",
    "        pad_len = max_len -len(tokens)\n",
    "        tokens = tokens + ([0]*pad_len) \n",
    "        \n",
    "        x = torch.tensor(tokens, dtype=torch.float).unsqueeze(1)\n",
    "        y = torch.tensor([label], dtype=torch.float).view(1, -1)\n",
    "        tokens_list.append(Data(x=x ,y=y))\n",
    "        \n",
    "    return tokens_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "880d072d-a270-47fb-b025-44bc53d31fbf",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "length of each tokens: 178\n",
      "Data(x=[178, 1], y=[1, 1])\n"
     ]
    }
   ],
   "source": [
    "tokens_list = smiles_to_token(data_list)\n",
    "for tokens in tokens_list:\n",
    "    print(f\"length of each tokens: {len(tokens.x)}\")\n",
    "    print(tokens)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "5caf561e-9d80-426a-9a70-fc929437fcc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 1\n",
    "train_dataset, valid_dataset, test_dataset = random_split(tokens_list, null_value=0, frac_train=0.8, frac_valid=0.1, frac_test=0.1, seed=seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "9c6f5e4a-72f8-4325-990d-cf687042da82",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 256\n",
    "train_loader = loader_dataset(data_list=train_dataset, batch_size=batch_size, shuffle=False)\n",
    "valid_loader = loader_dataset(data_list=valid_dataset, batch_size=batch_size, shuffle=False) \n",
    "test_loader = loader_dataset(data_list=test_dataset, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "48601bd2-bf59-425b-8cff-a3d0eabbd116",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataBatch(x=[45568, 1], y=[256, 1], batch=[45568], ptr=[257])\n"
     ]
    }
   ],
   "source": [
    "# loader 확인\n",
    "for batch in train_loader:\n",
    "    print(batch)\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19c6f714-dc8c-4ba6-97d3-a4a3849d8bc2",
   "metadata": {},
   "source": [
    "### Fingerprint로 변환"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "206d94e2-8417-4126-a157-73e64265e44a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# choice : 'rdkit', 'maccs', 'morgan' 중 변환할  fingerprint 선택\n",
    "def smiles_to_fingerprint(data_list, choice):\n",
    "    fp_list = []\n",
    "    for data in data_list:\n",
    "        smiles = data['smiles']\n",
    "        label = data['label']\n",
    "        \n",
    "        molecule = Chem.MolFromSmiles(smiles)\n",
    "        \n",
    "        if molecule is None :\n",
    "            continue\n",
    "        else:     \n",
    "            if choice == 'rdkit':\n",
    "                rdkit_fp = Chem.RDKFingerprint(molecule)\n",
    "                x = rdkit_fp\n",
    "            elif choice == 'maccs':\n",
    "                maccs_fp = MACCSkeys.GenMACCSKeys(molecule)\n",
    "                x = maccs_fp\n",
    "            elif choice == 'morgan':\n",
    "                morgan_fp = AllChem.GetMorganFingerpirntAsBitVect(molecule, radius=2, nBits=1024)\n",
    "                x = morgan_fp\n",
    "                \n",
    "            x = torch.tensor(x, dtype=torch.float).unsqueeze(1)\n",
    "            y = torch.tensor([label], dtype=torch.float).view(1, -1)\n",
    "            fp_list.append(Data(x=x, y=y))\n",
    "                           \n",
    "    return fp_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "b2206618-ecf1-410a-a90e-c9fa08fce484",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 데이터 불러오기\n",
    "fp_list = smiles_to_fingerprint(data_list, 'rdkit')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "638030d6-0711-46fd-8b80-3a7c36a87652",
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 1\n",
    "train_dataset, valid_dataset, test_dataset = random_split(fp_list, null_value=0, frac_train=0.8, frac_valid=0.1, frac_test=0.1, seed=seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "a14ef7d9-af65-437a-9b88-0c7ef720484d",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 256\n",
    "train_loader = loader_dataset(data_list=train_dataset, batch_size=batch_size, shuffle=False)\n",
    "valid_loader = loader_dataset(data_list=valid_dataset, batch_size=batch_size, shuffle=False) \n",
    "test_loader = loader_dataset(data_list=test_dataset, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "0dc5de8c-981a-41d2-bcb6-6dfdd1489b67",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataBatch(x=[524288, 1], y=[256, 1], batch=[524288], ptr=[257])\n"
     ]
    }
   ],
   "source": [
    "# loader 확인\n",
    "for batch in train_loader:\n",
    "    print(batch)\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95367093-0182-4605-a254-d9fc721c99b9",
   "metadata": {},
   "source": [
    "### Descriptors로 변환"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "25570a5e-96b2-442f-ad22-b8bd2b16af15",
   "metadata": {},
   "outputs": [],
   "source": [
    "def smiles_to_descriptors(data_list):\n",
    "\n",
    "    des_list = []\n",
    "    for data in data_list:\n",
    "        smiles = data['smiles']\n",
    "        label = data['label']\n",
    "        \n",
    "        molecule = Chem.MolFromSmiles(smiles)\n",
    "\n",
    "        # filtering\n",
    "        if molecule is None :\n",
    "            continue\n",
    "        else:\n",
    "            descriptors_dict= Descriptors.CalcMolDescriptors(molecule)\n",
    "            descriptor_vec = np.array([value for value in descriptors_dict.values()]) # dictionary의 value만 추출하여 vector 생성\n",
    "            x = torch.tensor(descriptor_vec, dtype=torch.float).unsqueeze(1)\n",
    "            y = torch.tensor([label], dtype=torch.float).view(1, -1)\n",
    "            des_list.append(Data(x=x, y=y))\n",
    "        \n",
    "    return des_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "1534d57e-303c-4dfa-8bae-cd6edd53118a",
   "metadata": {},
   "outputs": [],
   "source": [
    "des_list = smiles_to_descriptors(data_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "23bd038e-1a7f-42e3-9d56-cdc56cf2ede4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Data(x=[210, 1], y=[1, 1])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "des_list[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "f59df6af-f738-453a-9c42-da91faa18c5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 1\n",
    "train_dataset, valid_dataset, test_dataset = random_split(des_list, null_value=0, frac_train=0.8, frac_valid=0.1, frac_test=0.1, seed=seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "9d94b66f-7aa8-4c23-8a64-99bb8aa2392d",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 256\n",
    "train_loader = loader_dataset(data_list=train_dataset, batch_size=batch_size, shuffle=False)\n",
    "valid_loader = loader_dataset(data_list=valid_dataset, batch_size=batch_size, shuffle=False) \n",
    "test_loader = loader_dataset(data_list=test_dataset, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "fae043f0-69cf-4565-b90f-c8f883e1db80",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataBatch(x=[53760, 1], y=[256, 1], batch=[53760], ptr=[257])\n"
     ]
    }
   ],
   "source": [
    "for batch in train_loader:\n",
    "    print(batch)\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "beb5b904-0bfd-477f-9d0a-f5f68a64f3bd",
   "metadata": {},
   "source": [
    "### 3D Graph로 변환"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "3e01e741-5616-4e1e-8061-5ff1d9151eb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def molecule_to_3d(data_list):\n",
    "    \n",
    "    graph3d_list = []\n",
    "    for data in data_list:\n",
    "        smiles = data['smiles']\n",
    "        label = data['label']\n",
    "        \n",
    "        molecule = Chem.MolFromSmiles(smiles)\n",
    "\n",
    "        # filtering\n",
    "        if molecule is None :\n",
    "            continue\n",
    "        else:              \n",
    "            atom_info = [(atom.GetIdx(), atom.GetSymbol()) for atom in molecule.GetAtoms()]             \n",
    "            status = rdDistGeom.EmbedMolecule(molecule)\n",
    "            \n",
    "            # 3d graph 변환 filtering\n",
    "            if status != 0: # 0이 아닌 경우 변환 실패\n",
    "                continue\n",
    "            else:\n",
    "                conf = molecule.GetConformer()\n",
    "                pos = np.array([conf.GetAtomPosition(idx) for idx, symbol in atom_info])\n",
    "            \n",
    "                graph_data = pyg_utils.from_smiles(smiles)\n",
    "                graph_data.pos = pos\n",
    "                graph_data.y = torch.tensor([label], dtype=torch.float).view(1, -1)\n",
    "                graph3d_list.append(graph_data)\n",
    "        \n",
    "    return graph3d_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "cc428e45-f661-4635-9023-c42a6ff15fb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 데이터 불러오기\n",
    "graph3d_list = molecule_to_3d(data_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "7d5b9462-93c5-4226-a742-fd8bca992f2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 1\n",
    "train_dataset, valid_dataset, test_dataset = random_split(graph3d_list, null_value=0, frac_train=0.8, frac_valid=0.1, frac_test=0.1, seed=seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "daa7a078-09f8-4d6b-a8f4-5a4eb2ea4635",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 256\n",
    "train_loader = loader_dataset(data_list=train_dataset, batch_size=batch_size, shuffle=False)\n",
    "valid_loader = loader_dataset(data_list=valid_dataset, batch_size=batch_size, shuffle=False) \n",
    "test_loader = loader_dataset(data_list=test_dataset, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "173cabc0-58ed-457d-9780-b7ab836418c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataBatch(x=[8588, 9], edge_index=[2, 18586], edge_attr=[18586, 3], smiles=[256], pos=[256], y=[256, 1], batch=[8588], ptr=[257])\n"
     ]
    }
   ],
   "source": [
    "for batch in train_loader:\n",
    "    print(batch)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46311298-9764-4ced-b686-63ae904a9a81",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

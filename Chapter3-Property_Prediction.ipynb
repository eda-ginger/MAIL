{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "039db339-50ad-4a62-a0f2-e89dff130846",
   "metadata": {},
   "source": [
    "# Chapter 3 에서 배울 내용\n",
    "\n",
    "1. **데이터 로더 제작**  \n",
    "   - 모델 학습을 위한 데이터 로더 설계 및 구현  \n",
    "   - 분자 데이터 분류를 위한 사전 지식 이해\n",
    "\n",
    "---\n",
    "2. **모델 제작**  \n",
    "    - 분자 특성 분류/회귀 예측을 위한 모델 아키텍처 설계 \n",
    "    - MLP, CNN, RNN, GNN 등 다양한 모델 활용 방법  \n",
    "    - 입력 데이터 처리 및 모델에 맞는 출력 형식 설정\n",
    "\n",
    "---\n",
    "3. **학습 및 평가**\n",
    "   - 모델 학습을 위한 손실 함수와 최적화 알고리즘 설정  \n",
    "   - 교차 검증, 정확도, 정밀도, 재현율, F1 스코어 등 다양한 평가 지표 활용  \n",
    "   - 과적합 방지를 위한 기법 (예: Dropout, 정규화, 데이터 증강 등)\n",
    "   - 모델 성능 평가 \n",
    "    \n",
    "---\n",
    "4. **결과 저장**\n",
    "    - 학습 로그 및 평가 결과 기록  \n",
    "    - 결과 시각화 및 분석 도구 사용 (예: Loss/Accuracy 그래프, confusion matrix 등)\n",
    "    - 모델을 실험적 환경에서 재사용할 수 있도록 파일 포맷으로 저장 (예: `.pt`, `.h5` 등) \n",
    "    \n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "ac4ddb8e-caeb-4584-8b37-41bb7a90deca",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from rdkit import Chem\n",
    "from rdkit.Chem import AllChem, Descriptors, rdDepictor, rdDistGeom, MACCSkeys, rdMolDescriptors\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.nn import Sequential, Linear, ReLU, Conv1d\n",
    "from torch.utils.data import Dataset\n",
    "from torch_geometric import utils as pyg_utils\n",
    "from torch_geometric.data import InMemoryDataset, download_url, extract_gz, Data, Batch\n",
    "from torch_geometric.loader import DataLoader\n",
    "import torch_geometric.nn as gnn\n",
    "from torch_geometric.nn import GCNConv, GINConv, GATConv, global_max_pool as gmp\n",
    "from torch_geometric.nn import global_add_pool\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score, average_precision_score\n",
    "from tabulate import tabulate"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bf86b5c-da09-4700-9d76-378eb4087c28",
   "metadata": {},
   "source": [
    "## 1) 데이터 로더 제작\n",
    "\n",
    "데이터 로더(Data Loader)는 머신러닝/딥러닝 모델 학습에서 데이터를 효율적으로 불러오고 처리하기 위한 도구다. \n",
    "\n",
    "일반적으로 `PyTorch`와 같은 프레임워크에서 제공되며, 대규모 데이터셋을 다룰 때 데이터를 배치 단위로 나누거나 전처리 작업을 자동화하는 데 사용된다.\n",
    "\n",
    "데이터 로더를 사용하는 이유는 크게 다음과 같다.\n",
    "\n",
    "---\n",
    "1. **미니배치 생성**  \n",
    "   - 데이터를 미니배치 단위로 나누어 처리하여 학습 속도와 메모리 효율성을 높임\n",
    "   - 배치 크기가 작으면 메모리 사용량이 적으나, 학습 시간이 길어질 수 있음\n",
    "   - 배치 크기가 크면 학습 속도가 빨라질 수 있으나, 메모리 사용량이 증가함\n",
    "\n",
    "2. **데이터 셔플링**  \n",
    "   - 학습 데이터 순서를 매 에포크마다 변경해 모델의 일반화 성능을 향상\n",
    "   - 학습 시 특정 순서나 패턴으로 인해 모델이 편향되거나 과적합되지 않도록 하기 위해 사용됨.\n",
    "   - 검증 및 테스트 데이터에서는 **shuffle을 사용하지 않음** (결과 재현성을 위해)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "619051fe-e66a-45dd-b436-6dbb30890aeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def loader_dataset(data_list, batch_size, shuffle=False):\n",
    "    \"\"\"\n",
    "    DataLoader로 변경\n",
    "    \"\"\"\n",
    "    collate = Batch.from_data_list(data_list)\n",
    "    loader = DataLoader(data_list, batch_size=batch_size, collate_fn=collate, shuffle=shuffle)\n",
    "\n",
    "    return loader"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1edda7c6-286b-4359-bdd8-675de34b42e0",
   "metadata": {},
   "source": [
    "본 튜토리얼에서는 ESOL 데이터셋을 예시로 사용한다.\n",
    "\n",
    "먼저, Chapter2에서 저장한 데이터들을 불러와 data loader를 생성한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "18d05896-fa45-4575-81bd-679b412e2e8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_dataloader(dataset_name, data_type='Graph', batch_size=256):\n",
    "    data_path = f'dataset/{dataset_name}/processed/'\n",
    "    train_dataset = torch.load(os.path.join(data_path, f'{dataset_name}_{data_type}_train.pt'))\n",
    "    valid_dataset = torch.load(os.path.join(data_path, f'{dataset_name}_{data_type}_valid.pt'))\n",
    "    test_dataset = torch.load(os.path.join(data_path, f'{dataset_name}_{data_type}_test.pt'))    \n",
    "\n",
    "    if data_type == 'Descriptors':  # Descriptors 데이터를 처리할 경우 scaling 필요\n",
    "        # Step 1: Descriptors 데이터에서 x 값들만 추출\n",
    "        raw_features_train = [data.x.view(-1).numpy() for data in train_dataset]\n",
    "        raw_features_valid = [data.x.view(-1).numpy() for data in valid_dataset]\n",
    "        raw_features_test = [data.x.view(-1).numpy() for data in test_dataset]\n",
    "        # print(raw_features_train.shape)\n",
    "\n",
    "        # Step 2: 스케일링을 위한 StandardScaler 적용\n",
    "        scaler = StandardScaler()\n",
    "        scaled_features_train = scaler.fit_transform(raw_features_train)  # 학습 데이터에 맞춰 스케일링\n",
    "        scaled_features_valid = scaler.transform(raw_features_valid)  # 검증 데이터는 fit하지 않고 transform만 적용\n",
    "        scaled_features_test = scaler.transform(raw_features_test)  # 테스트 데이터도 동일\n",
    "\n",
    "        # Step 3: 스케일링된 데이터를 원본 데이터셋에 반영\n",
    "        for i, data in enumerate(train_dataset):\n",
    "            data.x = torch.tensor(scaled_features_train[i], dtype=torch.float).view(1, -1)\n",
    "        for i, data in enumerate(valid_dataset):\n",
    "            data.x = torch.tensor(scaled_features_valid[i], dtype=torch.float).view(1, -1)\n",
    "        for i, data in enumerate(test_dataset):\n",
    "            data.x = torch.tensor(scaled_features_test[i], dtype=torch.float).view(1, -1)       \n",
    "\n",
    "    train_loader = loader_dataset(data_list=train_dataset, batch_size=batch_size, shuffle=True)\n",
    "    valid_loader = loader_dataset(data_list=valid_dataset, batch_size=batch_size, shuffle=False) \n",
    "    test_loader = loader_dataset(data_list=test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "    return train_loader, valid_loader, test_loader"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fedc05b-8092-41d7-b129-cc5749baaf6a",
   "metadata": {},
   "source": [
    "`dataset_name` : 불러올 데이터셋의 이름 입력\n",
    "\n",
    "`data_type` : Chapter2 에서 저장한 데이터셋 유형 (Token, Fingerprint, Descriptor, Graph) 중 하나를 선택\n",
    "\n",
    "`batch_size` : 한번에 처리할 샘플 개수를 지정"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d35d9feb-50b7-4ca8-b21d-b164bd6880df",
   "metadata": {},
   "source": [
    " Graph loader가 어떻게 구성되어 있는지 확인해본다"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d22cdf9-95f6-44fc-b560-35ada156618a",
   "metadata": {},
   "source": [
    "### Graph Loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "44f70bdd-14d0-452b-bafd-0c03dcee9008",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataBatch(x=[4209, 9], edge_index=[2, 9110], edge_attr=[9110, 3], smiles=[128], y=[128, 1], batch=[4209], ptr=[129])\n"
     ]
    }
   ],
   "source": [
    "train_loader, valid_loader, test_loader = get_dataloader(dataset_name = 'bace', data_type='Graph', batch_size = 128)\n",
    "for data in train_loader:\n",
    "    print(data)\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "465530c0-e652-40e2-85c1-0655920e18e0",
   "metadata": {},
   "source": [
    "- x: 그래프의 노드 feature\n",
    "- edge_index: 그래프의 두 노드 간 연결(edge) 정보\n",
    "- edge_attr : 노드 간 연결(edge)의 feature\n",
    "- smiles : 그래프가 표현하는 분자의 smiles\n",
    "- y : task에 대한 label\n",
    "- batch : batch 내의 총 노드 개수\n",
    "- ptr : 각 그래프의 시작 노드의 위치. 그래프 개수(batch size) + 1 의 길이이며, 마지막 값은 batch에 포함된 전체 노드의 개수를 의미."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad728f0f-5fe3-417a-b1d0-19e185789a19",
   "metadata": {},
   "source": [
    "다른 표현형들의 경우 feature는 x에, label은 y에 저장되어있다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80134772-9fea-4878-8379-8dd8dca2b32d",
   "metadata": {},
   "source": [
    "## 2) 모델 제작\n",
    "\n",
    "분자의 표현형에 따라 데이터 타입이 달라지므로, 각기 다른 모델을 사용하여야 한다.\n",
    "\n",
    "예를 들어 Graph 형태의 데이터는 Graph를 처리할 수 있는 GNN을, String Tokenization 형태의 경우 순서 정보를 반영할 수 있는 1D CNN, RNN 등을 사용할 수 있다.\n",
    "\n",
    "가장 기본적인 모델인 MLP를 먼저 사용해보자\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09dc49a6-887d-462f-b528-434774318f1c",
   "metadata": {},
   "source": [
    "1. Fingerprint\n",
    "\n",
    "- Fingerprint는 분자의 특성을 단순히 vector 형태로 나타낸 것이다.\n",
    "- 따라서 기본 모델인 MLP로 학습하는 것이 적합하다.\n",
    "- 아래에 정의한 MLP 차원 및 layer 개수는 예시이므로, 데이터셋에 적합하게 수정하여 사용한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "28f8637b-c6a7-4e5f-a7a7-16698405ccd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 모델 \n",
    "class MLP(nn.Module): \n",
    "    def __init__(self, input_dim, hidden_dim, num_classes, dropout_rate=0.5): \n",
    "        super(MLP, self).__init__() \n",
    "        self.layer1 = nn.Linear(input_dim, hidden_dim) # input feature가 4개 \n",
    "        self.layer2 = nn.Linear(hidden_dim,hidden_dim*2)\n",
    "        self.layer3 = nn.Linear(hidden_dim*2, num_classes) \n",
    "        self.relu = nn.ReLU()\n",
    "        self.dropout = nn.Dropout(dropout_rate)\n",
    "        \n",
    "    def forward(self,data): # 모델에서 실행되어야 하는 계산을 정의\n",
    "        x = self.layer1(data.x.float()) # data의 feature가 들어있는 x 만 받아옴, 연산 시 데이터 타입을 일치시키기 위해 float() \n",
    "        x = self.relu(x)\n",
    "        x = self.dropout(x)  \n",
    "        x = self.layer2(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.dropout(x)  \n",
    "        x = self.layer3(x)   # 출력층\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb80a3ac-f972-4578-a51e-ed3ca3060854",
   "metadata": {},
   "source": [
    "- `input dim` : 모델의 입력 차원을 의미하며, 데이터 feature의 차원에 따라 결정되므로 적용하려는 데이터에 맞게 설정한다. \n",
    "\n",
    "- `hidden dim` : 입력 층을 거친 후 은닉층의 차원을 설정하는 부분이다. 이 파라미터와 layer 개수에 따라 모델의 capacity가 결정된다.\n",
    "\n",
    "- `num_classes` : 분류할 클래스의 개수를 나타낸다.\n",
    "\n",
    "- `dropout_rate` : dropout은 일부 뉴런을 랜덤으로 비활성화 하여 과적합을 방지하기 위한 기법이다. 이 파라미터는 비활성화 할 뉴런의 비율을 결정한다.\n",
    "\n",
    "- 각 레이어 사이에 비선형성을 추가하기 위해 활성화 함수가 포함된다.(예시에서는 ReLU사용)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "학습 전, 입력 차원을 결정하기 위해 data loader에서 모델에 학습할 정보를 담고 있는 x의 shape를 확인한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "34bf5c1b-4fa0-4fea-b466-066b756cdd5e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataBatch(x=[128, 1024], y=[128, 1], smiles=[128], batch=[128], ptr=[129])\n"
     ]
    }
   ],
   "source": [
    "# Fingerprint Loader 확인\n",
    "train_loader, valid_loader, test_loader = get_dataloader(dataset_name = 'bace', data_type='Fingerprint', batch_size = 128)\n",
    "for data in train_loader:\n",
    "    print(data)\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5a6c2c6-4e92-495c-a1ef-06237c3014f2",
   "metadata": {},
   "source": [
    "데이터의 입력을 나타내는 x의 차원이 1024이므로 `input_dim` = 1024가 된다. 또한 bace는 이진 분류이므로 `num_classes` = 1로 설정할 수 있다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6f9f9266-3511-426f-b351-9d926252adc0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MLP(\n",
       "  (layer1): Linear(in_features=1024, out_features=128, bias=True)\n",
       "  (layer2): Linear(in_features=128, out_features=256, bias=True)\n",
       "  (layer3): Linear(in_features=256, out_features=1, bias=True)\n",
       "  (relu): ReLU()\n",
       "  (dropout): Dropout(p=0.5, inplace=False)\n",
       ")"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fp_model1 = MLP(input_dim=1024, hidden_dim = 128, num_classes = 1, dropout_rate=0.5)\n",
    "fp_model1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fbc719c-4792-4b0d-95af-7ca249c51b39",
   "metadata": {},
   "source": [
    "임의로 설정된 `hidden_dim` 및 `dropout_rate`을 자유롭게 설정하여 모델을 만들어보자"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6ae49930-16c0-4c2e-9c20-e9f0ef3f24ab",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MLP(\n",
       "  (layer1): Linear(in_features=1024, out_features=32, bias=True)\n",
       "  (layer2): Linear(in_features=32, out_features=64, bias=True)\n",
       "  (layer3): Linear(in_features=64, out_features=1, bias=True)\n",
       "  (relu): ReLU()\n",
       "  (dropout): Dropout(p=0.7, inplace=False)\n",
       ")"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fp_model2 = MLP(input_dim=1024, hidden_dim = 32, num_classes = 1, dropout_rate=0.7)\n",
    "fp_model2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b67c64d0-b8e0-4308-996a-40f221f00589",
   "metadata": {},
   "source": [
    "2. Descriptors\n",
    "\n",
    "- Descriptors 역시 분자의 특성을 vector 형태로 나타낸 것이므로 MLP를 사용하여 학습한다.\n",
    "- 사전 정의한 MLP 모델에서 입력 차원만을 변경하여 사용한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ecd05469-c360-47aa-b239-967d00bee509",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataBatch(x=[128, 210], y=[128, 1], smiles=[128], batch=[128], ptr=[129])\n"
     ]
    }
   ],
   "source": [
    "# Descriptors Loader 확인\n",
    "train_loader, valid_loader, test_loader = get_dataloader(dataset_name = 'bace', data_type='Descriptors', batch_size = 128)\n",
    "for data in train_loader:\n",
    "    print(data)\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86863ae3-7318-4267-8449-68f9afa8bf69",
   "metadata": {},
   "source": [
    "descriptor의 x 차원은 210이므로 `input_dim` = 210이 된다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c230a3d0-667a-4be3-8d96-2347b908423e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MLP(\n",
       "  (layer1): Linear(in_features=210, out_features=128, bias=True)\n",
       "  (layer2): Linear(in_features=128, out_features=256, bias=True)\n",
       "  (layer3): Linear(in_features=256, out_features=1, bias=True)\n",
       "  (relu): ReLU()\n",
       "  (dropout): Dropout(p=0.2, inplace=False)\n",
       ")"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds_model = MLP(input_dim=210, hidden_dim = 128, num_classes = 1, dropout_rate=0.2)\n",
    "ds_model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "455662e3-5149-4264-86ad-34b37b2f608c",
   "metadata": {},
   "source": [
    "3. Token\n",
    "\n",
    "- Token은 분자를 문자열로 나타낸 데이터 형태이다.\n",
    "- 따라서 문자열의 순서를 반영할 수 있는 1D CNN을 사용하여 학습해본다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ab2c3929-ae87-4499-835a-bd3893a47dcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Conv1d(nn.Module):\n",
    "    def __init__(self, vocab_num, emb_dim, hidden_size, kernel_size, num_classes):\n",
    "        super(Conv1d, self).__init__()\n",
    "        \n",
    "        # Embedding Layer\n",
    "        self.embedding = nn.Embedding(vocab_num, emb_dim)\n",
    "        \n",
    "        # Conv1d Layers\n",
    "        self.layer1 = nn.Sequential(\n",
    "            nn.Conv1d(in_channels=emb_dim, out_channels=hidden_size, kernel_size=kernel_size, stride=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool1d(kernel_size=2)\n",
    "        )\n",
    "        \n",
    "        self.layer2 = nn.Sequential(\n",
    "            nn.Conv1d(in_channels=hidden_size, out_channels=hidden_size // 2, kernel_size=kernel_size, stride=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool1d(kernel_size=2)\n",
    "        )\n",
    "        \n",
    "        self.layer3 = nn.Sequential(\n",
    "            nn.Conv1d(in_channels=hidden_size // 2, out_channels=hidden_size // 4, kernel_size=kernel_size, stride=1),\n",
    "            nn.ReLU(),\n",
    "            nn.AdaptiveMaxPool1d(1)  # Ensure fixed output size\n",
    "        )\n",
    "        \n",
    "        # Fully Connected Layer\n",
    "        self.fc = nn.Linear(hidden_size // 4, num_classes)\n",
    "        nn.init.xavier_uniform_(self.fc.weight)  # Xavier initialization\n",
    "\n",
    "    def forward(self, data):\n",
    "        # Embedding Layer\n",
    "        x = self.embedding(data.x)  # Input shape: (batch_size, seq_len)\n",
    "        \n",
    "        # Permute to match Conv1d input format\n",
    "        x = x.permute(0, 2, 1)  # (batch_size, seq_len, emb_dim) -> (batch_size, emb_dim, seq_len)\n",
    "        \n",
    "        # Conv1d Layers\n",
    "        x = self.layer1(x)  # (batch_size, hidden_size, new_seq_len)\n",
    "        x = self.layer2(x)  # (batch_size, hidden_size // 2, smaller_seq_len)\n",
    "        x = self.layer3(x)  # (batch_size, hidden_size // 4, 1)\n",
    "        \n",
    "        # Flatten\n",
    "        x = x.view(x.size(0), -1)  # (batch_size, hidden_size // 4)\n",
    "        \n",
    "        # Fully Connected Layer\n",
    "        x = self.fc(x)  # (batch_size, 1)\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "648afcbf-6376-4605-b264-5085a5e8b70c",
   "metadata": {},
   "source": [
    "- `vocab_num` : 문자열을 구성하는 단어의 종류를 입력한다. chapter2에서 string tokenization 단계에서 출력한 단어집에서 단어의 종류를 알 수 있다. bace의 경우 Unkown 포함 39개가 있다.\n",
    "\n",
    "- `emb_dim` : 입력된 단어가 임베딩 될 차원을 결정한다.\n",
    "\n",
    "- `hidden_dim` : 임베딩 된 단어를 학습 시킬 때 은닉층의 차원을 설정한다.\n",
    "\n",
    "- `kernel_size` : kernel이 한 번에 읽을 시퀀스의 길이를 정의하며, 작을 수록 세밀한 패턴을, 클 수록 넓은 문맥을 포착한다.\n",
    "\n",
    "- `num_classes` : 분류할 클래스의 개수를 나타낸다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "04c821c7-048a-4470-8622-0b13cf821ea6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataBatch(x=[128, 178], y=[128, 1], smiles=[128], batch=[128], ptr=[129])\n"
     ]
    }
   ],
   "source": [
    "# Token Loader 확인\n",
    "train_loader, valid_loader, test_loader = get_dataloader(dataset_name = 'bace', data_type='Token', batch_size = 128)\n",
    "for data in train_loader:\n",
    "    print(data)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "6351be38-70ec-4641-a672-30a500c14eb6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Conv1d(\n",
       "  (embedding): Embedding(39, 128)\n",
       "  (layer1): Sequential(\n",
       "    (0): Conv1d(128, 128, kernel_size=(1,), stride=(1,))\n",
       "    (1): ReLU()\n",
       "    (2): MaxPool1d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  )\n",
       "  (layer2): Sequential(\n",
       "    (0): Conv1d(128, 64, kernel_size=(1,), stride=(1,))\n",
       "    (1): ReLU()\n",
       "    (2): MaxPool1d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  )\n",
       "  (layer3): Sequential(\n",
       "    (0): Conv1d(64, 32, kernel_size=(1,), stride=(1,))\n",
       "    (1): ReLU()\n",
       "    (2): AdaptiveMaxPool1d(output_size=1)\n",
       "  )\n",
       "  (fc): Linear(in_features=32, out_features=1, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = Conv1d(vocab_num=39, emb_dim=128, hidden_size = 128, kernel_size = 1, num_classes=1)\n",
    "model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c776c95-cdef-4e51-8a2e-1fe568ec374e",
   "metadata": {},
   "source": [
    "4. Graph\n",
    "\n",
    "- Graph는 분자를 구성하는 각 원자를 노드(node), 원자들의 결합을 엣지(edge)로 나타낸 데이터 형태이다.\n",
    "- Graph 형태의 데이터를 처리할 수 있는 신경망인 Graph Neural Network(GNN)을 사용하여야 한다.\n",
    "- 또한 GNN은 GCN, GIN, GAT 등 다양한 종류가 있으므로 하나씩 다루어보도록 한다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13fc9a78-9705-46bf-be00-143ea9f5203a",
   "metadata": {},
   "source": [
    "- GNN의 입력에는 노드의 특징 차원이 필요하다. data loader를 print하여 노드 x의 feature 차원을 출력해보자"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "2a780b7e-0e2e-4d4a-9bcf-948d899b9f52",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataBatch(x=[4316, 9], edge_index=[2, 9298], edge_attr=[9298, 3], smiles=[128], y=[128, 1], batch=[4316], ptr=[129])\n"
     ]
    }
   ],
   "source": [
    "# Descriptors Loader 확인\n",
    "train_loader, valid_loader, test_loader = get_dataloader(dataset_name = 'bace', data_type='Graph', batch_size = 128)\n",
    "for data in train_loader:\n",
    "        print(data)\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eec57a2c-74cd-46af-a0fa-69dfe65fc03f",
   "metadata": {},
   "source": [
    "- GNN은 각 노드들의 정보 전파 및 종합이 일어나는 gnn layer와, gnn layer가 생성한 표현을 바탕으로 분류 및 회귀 작업이 일어나는 feed forward network(ffn)로 구성된다. 따라서 `hidden_dim`은 gnn의 은닉층 차원을, `ffn_hidden`은 ffn layer의 은닉층의 차원을 지정하는 파라미터이다.\n",
    "- `input_dim` : 노드 feature의 차원을 입력한다.\n",
    "- `num_classes`: ffn layer의 마지막 차원을 결정하는 파라미터이므로, 분류하고자 하는 클래스의 개수를 입력한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "73481dc4-a7f8-46ab-8cdd-5ba515c568be",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GCNNet(torch.nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim = 64, ffn_hidden=64, num_classes=1, dropout_rate=0.2):\n",
    "\n",
    "        super(GCNNet, self).__init__()\n",
    "\n",
    "        # SMILES graph branch\n",
    "        self.num_classes = num_classes\n",
    "        self.conv1 = GCNConv(input_dim, hidden_dim)\n",
    "        self.conv2 = GCNConv(hidden_dim, hidden_dim*2)\n",
    "        # self.conv3 = GCNConv(hidden_dim*2, hidden_dim * 4)\n",
    "        # self.fc_g1 = torch.nn.Linear(hidden_dim*4, hidden_dim*2)\n",
    "        self.fc_g1 = torch.nn.Linear(hidden_dim*2, hidden_dim)\n",
    "        self.fc_g2 = torch.nn.Linear(hidden_dim, num_classes)\n",
    "\n",
    "        # activation\n",
    "        self.relu = nn.ReLU()\n",
    "        self.dropout = nn.Dropout(dropout_rate)\n",
    "\n",
    "    def forward(self, data):\n",
    "        # get graph input\n",
    "        x, edge_index, batch = data.x.float(), data.edge_index, data.batch\n",
    "\n",
    "        x = self.conv1(x, edge_index)\n",
    "        x = self.relu(x)\n",
    "\n",
    "        x = self.conv2(x, edge_index)\n",
    "        x = self.relu(x)\n",
    "\n",
    "        # x = self.conv3(x, edge_index)\n",
    "        # x = self.relu(x)\n",
    "        x = gmp(x, batch)       # global max pooling\n",
    "\n",
    "        # ffn layer\n",
    "        x = self.fc_g1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.dropout(x)\n",
    "        x = self.fc_g2(x)\n",
    "        out = self.dropout(x)\n",
    "        \n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "944a304d-e959-4c9b-8490-5d4d8013cc47",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GCNNet(\n",
       "  (conv1): GCNConv(9, 64)\n",
       "  (conv2): GCNConv(64, 128)\n",
       "  (fc_g1): Linear(in_features=128, out_features=64, bias=True)\n",
       "  (fc_g2): Linear(in_features=64, out_features=1, bias=True)\n",
       "  (relu): ReLU()\n",
       "  (dropout): Dropout(p=0.2, inplace=False)\n",
       ")"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "g_model1 = GCNNet(input_dim=9)\n",
    "g_model1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "cb3d272b-209a-458d-8fbd-4e643c38e078",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GATNet(torch.nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim = 64, ffn_hidden=128, num_classes=1, dropout_rate=0.2):\n",
    "        super(GATNet, self).__init__()\n",
    "\n",
    "        # graph layers\n",
    "        self.gcn1 = GATConv(input_dim, hidden_dim, heads=10, dropout=dropout_rate)\n",
    "        self.gcn2 = GATConv(hidden_dim*10, hidden_dim*2, dropout=dropout_rate)\n",
    "        self.fc_g1 = nn.Linear(hidden_dim*2, ffn_hidden)\n",
    "        self.fc_g2 = nn.Linear(ffn_hidden, num_classes)\n",
    "\n",
    "        # activation and regularization\n",
    "        self.relu = nn.ReLU()\n",
    "        self.dropout = nn.Dropout(dropout_rate)\n",
    "\n",
    "    def forward(self, data):\n",
    "        # graph input feed-forward\n",
    "        x, edge_index, batch = data.x.float(), data.edge_index, data.batch\n",
    "\n",
    "        x = F.dropout(x, p=0.2, training=self.training)\n",
    "        x = F.elu(self.gcn1(x, edge_index))\n",
    "        x = F.dropout(x, p=0.2, training=self.training)\n",
    "        x = self.gcn2(x, edge_index)\n",
    "        x = self.relu(x)\n",
    "        x = gmp(x, batch)          # global max pooling\n",
    "\n",
    "        # ffn layer\n",
    "        x = self.fc_g1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.dropout(x)\n",
    "        x = self.fc_g2(x)\n",
    "        out = self.dropout(x)\n",
    "        \n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "0ed37126-8464-4fd4-b66e-31ee0599fa70",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GATNet(\n",
       "  (gcn1): GATConv(9, 64, heads=10)\n",
       "  (gcn2): GATConv(640, 128, heads=1)\n",
       "  (fc_g1): Linear(in_features=128, out_features=128, bias=True)\n",
       "  (fc_g2): Linear(in_features=128, out_features=1, bias=True)\n",
       "  (relu): ReLU()\n",
       "  (dropout): Dropout(p=0.2, inplace=False)\n",
       ")"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "g_model2 = GATNet(input_dim=9)\n",
    "g_model2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "b277eed1-2465-4aab-a2c7-f675c110f1f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GINConvNet(torch.nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim = 128, ffn_hidden=128, num_classes=1, dropout_rate=0.2):\n",
    "        super(GINConvNet, self).__init__()\n",
    "\n",
    "        dim = 32\n",
    "        self.dropout = nn.Dropout(dropout_rate)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.num_classes = num_classes\n",
    "        # convolution layers\n",
    "        nn1 = Sequential(Linear(input_dim, dim), ReLU(), Linear(dim, dim))\n",
    "        self.conv1 = GINConv(nn1)\n",
    "        self.bn1 = torch.nn.BatchNorm1d(dim)\n",
    "\n",
    "        nn2 = Sequential(Linear(dim, dim), ReLU(), Linear(dim, dim))\n",
    "        self.conv2 = GINConv(nn2)\n",
    "        self.bn2 = torch.nn.BatchNorm1d(dim)\n",
    "\n",
    "        nn3 = Sequential(Linear(dim, dim), ReLU(), Linear(dim, dim))\n",
    "        self.conv3 = GINConv(nn3)\n",
    "        self.bn3 = torch.nn.BatchNorm1d(dim)\n",
    "\n",
    "        nn4 = Sequential(Linear(dim, dim), ReLU(), Linear(dim, dim))\n",
    "        self.conv4 = GINConv(nn4)\n",
    "        self.bn4 = torch.nn.BatchNorm1d(dim)\n",
    "\n",
    "        nn5 = Sequential(Linear(dim, dim), ReLU(), Linear(dim, dim))\n",
    "        self.conv5 = GINConv(nn5)\n",
    "        self.bn5 = torch.nn.BatchNorm1d(dim)\n",
    "\n",
    "        self.fc = Linear(dim, hidden_dim)\n",
    "        \n",
    "        self.fc_g1 = nn.Linear(hidden_dim, ffn_hidden)\n",
    "        self.fc_g2 = nn.Linear(ffn_hidden, self.num_classes)\n",
    "    \n",
    "\n",
    "    def forward(self, data):\n",
    "        x, edge_index, batch = data.x, data.edge_index, data.batch\n",
    "\n",
    "        x = F.relu(self.conv1(x, edge_index))\n",
    "        x = self.bn1(x)\n",
    "        x = F.relu(self.conv2(x, edge_index))\n",
    "        x = self.bn2(x)\n",
    "        x = F.relu(self.conv3(x, edge_index))\n",
    "        x = self.bn3(x)\n",
    "        x = F.relu(self.conv4(x, edge_index))\n",
    "        x = self.bn4(x)\n",
    "        x = F.relu(self.conv5(x, edge_index))\n",
    "        x = self.bn5(x)\n",
    "        \n",
    "        x = global_add_pool(x, batch)\n",
    "        x = F.relu(self.fc(x))\n",
    "        x = F.dropout(x, p=0.2, training=self.training)\n",
    "\n",
    "        # add some dense layers\n",
    "        x = self.fc_g1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.dropout(x)\n",
    "\n",
    "        x = self.fc_g2(x)\n",
    "        x = self.relu(x)\n",
    "        out = self.dropout(x)\n",
    "        \n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "096b6f3b-ed35-49f2-a997-412fce996fcf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GINConvNet(\n",
       "  (dropout): Dropout(p=0.2, inplace=False)\n",
       "  (relu): ReLU()\n",
       "  (conv1): GINConv(nn=Sequential(\n",
       "    (0): Linear(in_features=9, out_features=32, bias=True)\n",
       "    (1): ReLU()\n",
       "    (2): Linear(in_features=32, out_features=32, bias=True)\n",
       "  ))\n",
       "  (bn1): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (conv2): GINConv(nn=Sequential(\n",
       "    (0): Linear(in_features=32, out_features=32, bias=True)\n",
       "    (1): ReLU()\n",
       "    (2): Linear(in_features=32, out_features=32, bias=True)\n",
       "  ))\n",
       "  (bn2): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (conv3): GINConv(nn=Sequential(\n",
       "    (0): Linear(in_features=32, out_features=32, bias=True)\n",
       "    (1): ReLU()\n",
       "    (2): Linear(in_features=32, out_features=32, bias=True)\n",
       "  ))\n",
       "  (bn3): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (conv4): GINConv(nn=Sequential(\n",
       "    (0): Linear(in_features=32, out_features=32, bias=True)\n",
       "    (1): ReLU()\n",
       "    (2): Linear(in_features=32, out_features=32, bias=True)\n",
       "  ))\n",
       "  (bn4): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (conv5): GINConv(nn=Sequential(\n",
       "    (0): Linear(in_features=32, out_features=32, bias=True)\n",
       "    (1): ReLU()\n",
       "    (2): Linear(in_features=32, out_features=32, bias=True)\n",
       "  ))\n",
       "  (bn5): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (fc): Linear(in_features=32, out_features=128, bias=True)\n",
       "  (fc_g1): Linear(in_features=128, out_features=128, bias=True)\n",
       "  (fc_g2): Linear(in_features=128, out_features=1, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "g_model3 = GINConvNet(input_dim=9)\n",
    "g_model3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40484308-61b2-443f-9702-aab31017ca2e",
   "metadata": {},
   "source": [
    "## 3) 학습 및 평가\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "d0652585-7ddd-470c-809c-629ed4bb4f10",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 학습 진행 함수 정의\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "def train(model, train_loader, valid_loader, epochs):\n",
    "    model = model.to(device)\n",
    "    # train_loader, valid_loader, test_loader = loaders\n",
    "    optimizer = torch.optim.SGD(model.parameters(), lr = 0.00001, momentum = 0.5) \n",
    "    criterion = nn.BCEWithLogitsLoss() \n",
    "\n",
    "    best_valid_loss = float('inf')  # 초기값을 무한대로 설정\n",
    "    best_epoch = 0  # 가장 성능 좋은 epoch을 기록\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        print(f\"Epoch {epoch + 1}/{epochs}\")\n",
    "        train_loss = 0.0\n",
    "        model.train() # 학습 모드 전환    \n",
    "\n",
    "        # train\n",
    "        for data in train_loader:\n",
    "            data = data.to(device)\n",
    "            y = data.y\n",
    "            \n",
    "            optimizer.zero_grad() # 전 단계에서의 loss gradient 값을 초기화\n",
    "            output = model(data)\n",
    "            loss = criterion(output,y)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            train_loss += loss.item()\n",
    "\n",
    "        print(f\"Training Loss: {train_loss / len(train_loader):.4f}\")\n",
    "\n",
    "        # validation\n",
    "        model.eval() # 학습 모드 전환    \n",
    "        valid_loss = 0.0\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        \n",
    "        with torch.no_grad():  # Gradient 계산 비활성화\n",
    "            for data in valid_loader:\n",
    "                data = data.to(device)\n",
    "                y = data.y\n",
    "                \n",
    "                output = model(data)\n",
    "                loss = criterion(output, y)\n",
    "                valid_loss += loss.item()\n",
    "\n",
    "                # Accuracy 계산\n",
    "                predicted = (F.sigmoid(output) > 0.5).int()\n",
    "\n",
    "                total += y.size(0)\n",
    "                correct += (predicted == y).sum().item()\n",
    "\n",
    "        accuracy = 100 * (correct / total)\n",
    "        avg_valid_loss = valid_loss / len(valid_loader)\n",
    "        print(f\"Validation Loss: {avg_valid_loss:.4f}, Validation Accuracy: {accuracy:.2f}%\\n\")\n",
    "        \n",
    "        # Best model 저장\n",
    "        if avg_valid_loss < best_valid_loss:\n",
    "            best_valid_loss = avg_valid_loss\n",
    "            best_epoch = epoch + 1\n",
    "            torch.save(model.state_dict(), \"best_epoch.pth\")\n",
    "            print(f\"Best model updated at epoch {best_epoch} with validation loss: {best_valid_loss:.4f}\")\n",
    "\n",
    "    print(f\"Training completed. Best model was at epoch {best_epoch} with validation loss: {best_valid_loss:.4f}\")\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a3067ff-3017-4f75-8756-86f8aad88b96",
   "metadata": {},
   "source": [
    "- `model` : 학습할 데이터셋에 맞게 설정한 모델 입력\n",
    "- `train_loader`, `valid_loader` : 학습, 검증 data loader 입력\n",
    "- `epochs` : 반복할 학습 횟수 입력"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "387bd47c-1401-4b66-b53e-f5c21f518b26",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "Training Loss: 0.7022\n",
      "Validation Loss: 0.6539, Validation Accuracy: 86.09%\n",
      "\n",
      "Best model updated at epoch 1 with validation loss: 0.6539\n",
      "Epoch 2/10\n",
      "Training Loss: 0.7028\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/torch/nn/functional.py:1967: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n",
      "  warnings.warn(\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss: 0.6539, Validation Accuracy: 86.09%\n",
      "\n",
      "Epoch 3/10\n",
      "Training Loss: 0.7024\n",
      "Validation Loss: 0.6539, Validation Accuracy: 86.09%\n",
      "\n",
      "Epoch 4/10\n",
      "Training Loss: 0.7037\n",
      "Validation Loss: 0.6539, Validation Accuracy: 86.09%\n",
      "\n",
      "Epoch 5/10\n",
      "Training Loss: 0.7024\n",
      "Validation Loss: 0.6539, Validation Accuracy: 86.09%\n",
      "\n",
      "Epoch 6/10\n",
      "Training Loss: 0.7021\n",
      "Validation Loss: 0.6540, Validation Accuracy: 86.09%\n",
      "\n",
      "Epoch 7/10\n",
      "Training Loss: 0.7012\n",
      "Validation Loss: 0.6540, Validation Accuracy: 86.09%\n",
      "\n",
      "Epoch 8/10\n",
      "Training Loss: 0.7011\n",
      "Validation Loss: 0.6540, Validation Accuracy: 86.09%\n",
      "\n",
      "Epoch 9/10\n",
      "Training Loss: 0.7017\n",
      "Validation Loss: 0.6540, Validation Accuracy: 86.09%\n",
      "\n",
      "Epoch 10/10\n",
      "Training Loss: 0.7024\n",
      "Validation Loss: 0.6540, Validation Accuracy: 86.09%\n",
      "\n",
      "Training completed. Best model was at epoch 1 with validation loss: 0.6539\n"
     ]
    }
   ],
   "source": [
    "# fingerprint loader 생성\n",
    "train_loader, valid_loader, test_loader = get_dataloader(dataset_name = 'bace', data_type='Fingerprint', batch_size = 128)\n",
    "\n",
    "# 학습할 모델 정의\n",
    "fp_model1 = MLP(input_dim=1024, hidden_dim = 128, num_classes = 1, dropout_rate=0.5)\n",
    "\n",
    "# train\n",
    "p = train(fp_model1, train_loader, valid_loader, epochs=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "773f3bf5-545f-453c-b737-3f10364b9995",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "Training Loss: 0.6944\n",
      "Validation Loss: 0.7093, Validation Accuracy: 34.44%\n",
      "\n",
      "Best model updated at epoch 1 with validation loss: 0.7093\n",
      "Epoch 2/10\n",
      "Training Loss: 0.6977\n",
      "Validation Loss: 0.7094, Validation Accuracy: 34.44%\n",
      "\n",
      "Epoch 3/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/torch/nn/functional.py:1967: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n",
      "  warnings.warn(\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Loss: 0.6951\n",
      "Validation Loss: 0.7094, Validation Accuracy: 34.44%\n",
      "\n",
      "Epoch 4/10\n",
      "Training Loss: 0.7051\n",
      "Validation Loss: 0.7095, Validation Accuracy: 34.44%\n",
      "\n",
      "Epoch 5/10\n",
      "Training Loss: 0.7032\n",
      "Validation Loss: 0.7096, Validation Accuracy: 34.44%\n",
      "\n",
      "Epoch 6/10\n",
      "Training Loss: 0.6993\n",
      "Validation Loss: 0.7096, Validation Accuracy: 33.77%\n",
      "\n",
      "Epoch 7/10\n",
      "Training Loss: 0.7085\n",
      "Validation Loss: 0.7097, Validation Accuracy: 33.77%\n",
      "\n",
      "Epoch 8/10\n",
      "Training Loss: 0.7050\n",
      "Validation Loss: 0.7097, Validation Accuracy: 33.77%\n",
      "\n",
      "Epoch 9/10\n",
      "Training Loss: 0.7028\n",
      "Validation Loss: 0.7098, Validation Accuracy: 33.77%\n",
      "\n",
      "Epoch 10/10\n",
      "Training Loss: 0.7012\n",
      "Validation Loss: 0.7098, Validation Accuracy: 33.77%\n",
      "\n",
      "Training completed. Best model was at epoch 1 with validation loss: 0.7093\n"
     ]
    }
   ],
   "source": [
    "# Descriptor loader 생성\n",
    "train_loader, valid_loader, test_loader = get_dataloader(dataset_name = 'bace', data_type='Descriptors', batch_size = 128)\n",
    "\n",
    "# 학습할 모델 정의\n",
    "ds_model = MLP(input_dim=210, hidden_dim = 128, num_classes = 1, dropout_rate=0.7)\n",
    "\n",
    "# train\n",
    "train(ds_model, train_loader, valid_loader, epochs=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "bea4f062-3e79-45be-b9a2-84e1a70c3b0f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "Training Loss: 0.6774\n",
      "Validation Loss: 0.9769, Validation Accuracy: 13.91%\n",
      "\n",
      "Best model updated at epoch 1 with validation loss: 0.9769\n",
      "Epoch 2/10\n",
      "Training Loss: 0.6769\n",
      "Validation Loss: 0.9768, Validation Accuracy: 13.91%\n",
      "\n",
      "Best model updated at epoch 2 with validation loss: 0.9768\n",
      "Epoch 3/10\n",
      "Training Loss: 0.6752\n",
      "Validation Loss: 0.9767, Validation Accuracy: 13.91%\n",
      "\n",
      "Best model updated at epoch 3 with validation loss: 0.9767\n",
      "Epoch 4/10\n",
      "Training Loss: 0.6778\n",
      "Validation Loss: 0.9766, Validation Accuracy: 13.91%\n",
      "\n",
      "Best model updated at epoch 4 with validation loss: 0.9766\n",
      "Epoch 5/10\n",
      "Training Loss: 0.6742\n",
      "Validation Loss: 0.9766, Validation Accuracy: 13.91%\n",
      "\n",
      "Best model updated at epoch 5 with validation loss: 0.9766\n",
      "Epoch 6/10\n",
      "Training Loss: 0.6762\n",
      "Validation Loss: 0.9765, Validation Accuracy: 13.91%\n",
      "\n",
      "Best model updated at epoch 6 with validation loss: 0.9765\n",
      "Epoch 7/10\n",
      "Training Loss: 0.6778\n",
      "Validation Loss: 0.9764, Validation Accuracy: 13.91%\n",
      "\n",
      "Best model updated at epoch 7 with validation loss: 0.9764\n",
      "Epoch 8/10\n",
      "Training Loss: 0.6746\n",
      "Validation Loss: 0.9764, Validation Accuracy: 13.91%\n",
      "\n",
      "Best model updated at epoch 8 with validation loss: 0.9764\n",
      "Epoch 9/10\n",
      "Training Loss: 0.6762\n",
      "Validation Loss: 0.9763, Validation Accuracy: 13.91%\n",
      "\n",
      "Best model updated at epoch 9 with validation loss: 0.9763\n",
      "Epoch 10/10\n",
      "Training Loss: 0.6779\n",
      "Validation Loss: 0.9762, Validation Accuracy: 13.91%\n",
      "\n",
      "Best model updated at epoch 10 with validation loss: 0.9762\n",
      "Training completed. Best model was at epoch 10 with validation loss: 0.9762\n"
     ]
    }
   ],
   "source": [
    "# Token loader 생성\n",
    "train_loader, valid_loader, test_loader = get_dataloader(dataset_name = 'bace', data_type='Token', batch_size = 128)\n",
    "\n",
    "# 학습할 모델 정의\n",
    "token_model = Conv1d(vocab_num=39, emb_dim=128, hidden_size = 128, kernel_size = 1, num_classes=1)\n",
    "\n",
    "# train\n",
    "train(token_model, train_loader, valid_loader, epochs=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "de45a937-ccb8-49ef-ba7b-98585b7bb162",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "Training Loss: 0.9844\n",
      "Validation Loss: 0.5003, Validation Accuracy: 86.09%\n",
      "\n",
      "Best model updated at epoch 1 with validation loss: 0.5003\n",
      "Epoch 2/10\n",
      "Training Loss: 0.9103\n",
      "Validation Loss: 0.3343, Validation Accuracy: 86.09%\n",
      "\n",
      "Best model updated at epoch 2 with validation loss: 0.3343\n",
      "Epoch 3/10\n",
      "Training Loss: 0.9384\n",
      "Validation Loss: 0.3036, Validation Accuracy: 86.09%\n",
      "\n",
      "Best model updated at epoch 3 with validation loss: 0.3036\n",
      "Epoch 4/10\n",
      "Training Loss: 0.8880\n",
      "Validation Loss: 0.3002, Validation Accuracy: 85.43%\n",
      "\n",
      "Best model updated at epoch 4 with validation loss: 0.3002\n",
      "Epoch 5/10\n",
      "Training Loss: 0.8842\n",
      "Validation Loss: 0.3285, Validation Accuracy: 80.79%\n",
      "\n",
      "Epoch 6/10\n",
      "Training Loss: 0.8785\n",
      "Validation Loss: 0.3714, Validation Accuracy: 71.52%\n",
      "\n",
      "Epoch 7/10\n",
      "Training Loss: 0.8923\n",
      "Validation Loss: 0.3800, Validation Accuracy: 69.54%\n",
      "\n",
      "Epoch 8/10\n",
      "Training Loss: 0.9000\n",
      "Validation Loss: 0.4012, Validation Accuracy: 68.87%\n",
      "\n",
      "Epoch 9/10\n",
      "Training Loss: 0.8707\n",
      "Validation Loss: 0.4213, Validation Accuracy: 69.54%\n",
      "\n",
      "Epoch 10/10\n",
      "Training Loss: 0.8507\n",
      "Validation Loss: 0.4424, Validation Accuracy: 70.86%\n",
      "\n",
      "Training completed. Best model was at epoch 4 with validation loss: 0.3002\n"
     ]
    }
   ],
   "source": [
    "# Graph loader 생성\n",
    "train_loader, valid_loader, test_loader = get_dataloader(dataset_name = 'bace', data_type='Graph', batch_size = 128)\n",
    "\n",
    "# 학습할 모델 정의\n",
    "g_model3 = GINConvNet(input_dim=9)\n",
    "\n",
    "# train\n",
    "train(g_model3, train_loader, valid_loader, epochs=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bafe9646-036b-4d22-9f7c-98595a3c1c15",
   "metadata": {},
   "source": [
    "학습된 모델로 평가를 수행한다.\n",
    "\n",
    "평가는 accuracy, precision, recall, f1_score, roc_auc, average precision 총 6가지로 이루어지며, 설명은 다음과 같다.\n",
    "\n",
    "- Accuracy (정확도): 올바르게 예측한 샘플의 비율\n",
    "\n",
    "- Precision (정밀도): 모델이 양성으로 예측한 샘플 중 실제로 양성인 비율.\n",
    "\n",
    "- Recall (재현율): 실제 양성 샘플 중 모델이 양성으로 예측한 비율.\n",
    "\n",
    "- F1-Score: Precision과 Recall의 조화 평균.데이터가 불균형할 때 모델의 전반적 성능을 파악하기 좋음.\n",
    "\n",
    "- AUC_ROC (ROC 곡선 아래 면적): 민감도(TPR)와 특이도(1-FPR) 간의 균형을 평가.1에 가까울수록 좋음.\n",
    "\n",
    "- AUC_PRC (PRC 곡선 아래 면적): Precision과 Recall 간의 관계를 시각화한 곡선의 면적."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "8ce32eb5-064b-46dc-9415-a27b9cbf5c62",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(model, test_loader, save_file = 'Results.csv'):\n",
    "    model = model.to(device)\n",
    "    model.eval()    \n",
    "    criterion = nn.BCEWithLogitsLoss() \n",
    "    \n",
    "    all_preds = []\n",
    "    all_targets = []\n",
    "    test_loss = 0.0\n",
    "    \n",
    "    with torch.no_grad():  # Gradient 계산 비활성화\n",
    "        for data in test_loader:\n",
    "            data = data.to(device)\n",
    "            y = data.y\n",
    "            \n",
    "            output = model(data)\n",
    "            preds = torch.sigmoid(output).cpu().numpy().ravel()  # Sigmoid로 확률 변환 후 numpy로 변환\n",
    "            all_preds.append(preds)\n",
    "            all_targets.append(y.cpu().numpy())\n",
    "            \n",
    "            # 손실 계산\n",
    "            test_loss += criterion(output, y).item()\n",
    "            \n",
    "        # Concatenate predictions and targets\n",
    "        all_preds = np.concatenate(all_preds)\n",
    "        all_targets = np.concatenate(all_targets).astype(int)\n",
    "    \n",
    "        # 메트릭 계산\n",
    "        test_acc = accuracy_score(all_targets, (all_preds >= 0.5).astype(int))\n",
    "        test_precision = precision_score(all_targets, (all_preds >= 0.5).astype(int))\n",
    "        test_recall = recall_score(all_targets, (all_preds >= 0.5).astype(int))\n",
    "        test_f1 = f1_score(all_targets, (all_preds >= 0.5).astype(int))\n",
    "        test_auc_roc = roc_auc_score(all_targets, all_preds)\n",
    "        test_auc_prc = average_precision_score(all_targets, all_preds)\n",
    "    \n",
    "        # 평균 손실 계산\n",
    "        test_loss /= len(test_loader)\n",
    "    \n",
    "        # 결과 저장\n",
    "        metrics = {\n",
    "            \"Loss\": test_loss,\n",
    "            \"Accuracy\": test_acc,\n",
    "            \"Precision\": test_precision,\n",
    "            \"Recall\": test_recall,\n",
    "            \"F1-Score\": test_f1,\n",
    "            \"AUC_ROC\": test_auc_roc,\n",
    "            \"AUC_PRC\": test_auc_prc\n",
    "        }\n",
    "    \n",
    "        # 결과 출력\n",
    "        print(\"\\n최적 모델의 테스트 데이터 성능:\")\n",
    "        print(tabulate(pd.DataFrame(metrics, index=[\"Metric Value\"]).T, headers=\"keys\", tablefmt=\"fancy_grid\"))\n",
    "    \n",
    "        # CSV 파일 저장\n",
    "        df = pd.DataFrame(metrics, index=[0])\n",
    "        df.to_csv(save_file, index=False)\n",
    "        print(f\"\\nResults saved to {save_file}\")\n",
    "\n",
    "    return metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "90bdbc98-1602-4828-8095-72f029622c17",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "최적 모델의 테스트 데이터 성능:\n",
      "╒═══════════╤════════════════╕\n",
      "│           │   Metric Value │\n",
      "╞═══════════╪════════════════╡\n",
      "│ Loss      │       0.548003 │\n",
      "├───────────┼────────────────┤\n",
      "│ Accuracy  │       0.532895 │\n",
      "├───────────┼────────────────┤\n",
      "│ Precision │       0.532895 │\n",
      "├───────────┼────────────────┤\n",
      "│ Recall    │       1        │\n",
      "├───────────┼────────────────┤\n",
      "│ F1-Score  │       0.695279 │\n",
      "├───────────┼────────────────┤\n",
      "│ AUC_ROC   │       0.663537 │\n",
      "├───────────┼────────────────┤\n",
      "│ AUC_PRC   │       0.671833 │\n",
      "╘═══════════╧════════════════╛\n",
      "\n",
      "Results saved to Results.csv\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'Loss': 0.5480027794837952,\n",
       " 'Accuracy': 0.5328947368421053,\n",
       " 'Precision': 0.5328947368421053,\n",
       " 'Recall': 1.0,\n",
       " 'F1-Score': 0.6952789699570815,\n",
       " 'AUC_ROC': 0.6635367762128326,\n",
       " 'AUC_PRC': 0.6718333041640333}"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# best model load\n",
    "g_model3 = GINConvNet(input_dim=9)\n",
    "g_model3.load_state_dict(torch.load(\"best_epoch.pth\"))\n",
    "test(g_model3, test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee5847bb-543a-4b51-93fe-c5f2d214fbc1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

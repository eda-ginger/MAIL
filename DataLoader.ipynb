{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9e156edc-63cf-4162-bf7d-ebf06f974434",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "from rdkit import RDLogger\n",
    "\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "RDLogger.DisableLog('rdApp.*')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f4f9afab-6502-4d12-8669-1f378cebe631",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No normalization for SPS. Feature removed!\n",
      "No normalization for AvgIpc. Feature removed!\n",
      "2024-11-30 16:32:01.098732: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2024-11-30 16:32:01.103520: I external/local_xla/xla/tsl/cuda/cudart_stub.cc:32] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2024-11-30 16:32:01.113874: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1732984321.131128   94737 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1732984321.136195   94737 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-11-30 16:32:01.154529: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /opt/conda/lib/python3.10/site-packages/tensorflow/python/util/deprecation.py:588: calling function (from tensorflow.python.eager.polymorphic_function.polymorphic_function) with experimental_relax_shapes is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "experimental_relax_shapes is deprecated, use reduce_retracing instead\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Skipped loading modules with pytorch-geometric dependency, missing a dependency. No module named 'dgl'\n",
      "Skipped loading modules with pytorch-lightning dependency, missing a dependency. No module named 'lightning'\n",
      "Skipped loading some Jax models, missing a dependency. No module named 'jax'\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import os\n",
    "from rdkit import Chem\n",
    "from rdkit.Chem import AllChem, Descriptors, rdDepictor, rdDistGeom, MACCSkeys, rdMolDescriptors\n",
    "from torch_geometric.data import InMemoryDataset, download_url, extract_gz, Data, DataLoader, Batch\n",
    "from torch.utils.data import Dataset\n",
    "from torch_geometric import utils as pyg_utils\n",
    "from deepchem.feat.smiles_tokenizer import BasicSmilesTokenizer\n",
    "\n",
    "# 필요한 별도 파일\n",
    "from splitters import random_split, scaffold_split ## split\n",
    "from download_preprocess import CustomMoleculeNet, atom_features, EDGE_FEATURES"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b185c499-ed3c-4a3b-9e8e-0079b2ae6f8c",
   "metadata": {},
   "source": [
    "### Data Download"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "65cbe79e-0a63-41dc-b2e9-ed102204e765",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading https://deepchemdata.s3-us-west-1.amazonaws.com/datasets/bace.csv\n",
      "Processing...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data processed and saved successfully.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Done!\n"
     ]
    }
   ],
   "source": [
    "# dataset 다운로드 - smiles와 label을 저장\n",
    "dataset_name = 'bace'\n",
    "dataset = CustomMoleculeNet('dataset', name=dataset_name.upper())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "53ff281a-20ae-4ce6-a1a5-0e2ebdc15fe7",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_list = torch.load('dataset/' + dataset_name + '/processed/smiles_labels.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "efe9f605-8b8d-4cd2-8d24-c5642905001b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1513,\n",
       " 'O1CC[C@@H](NC(=O)[C@@H](Cc2cc3cc(ccc3nc2N)-c2ccccc2C)C)CC1(C)C',\n",
       " [[1.0]])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(data_list), data_list[0]['smiles'], data_list[0]['label']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8e6779b-da3b-4767-9e0f-1b2abe752ccb",
   "metadata": {},
   "source": [
    "### Graph Data로 변환"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5c0c4d2e-93f8-455c-97f4-7a4156cbc5ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# smiles를 graph data로 변환\n",
    "def smiles_to_graph(data_list: list, with_hydrogen: bool = False, kekulize: bool = False) :\n",
    "\n",
    "    graph_list = []\n",
    "    for data in data_list:\n",
    "        smiles = data['smiles']\n",
    "        label = data['label']\n",
    "        \n",
    "        mol = Chem.MolFromSmiles(smiles)\n",
    "        \n",
    "        # smiles가 객체로 변환되지 않는 경우 filtering\n",
    "        if mol is None :\n",
    "            continue\n",
    "        else: \n",
    "            # mol -> graph\n",
    "            if with_hydrogen:\n",
    "                mol = Chem.AddHs(mol)\n",
    "            if kekulize:\n",
    "                Chem.Kekulize(mol)\n",
    "        \n",
    "            xs: List[List[int]] = []\n",
    "            for atom in mol.GetAtoms():\n",
    "                current_atom_feat = atom_features(atom)\n",
    "                xs.append(current_atom_feat)\n",
    "        \n",
    "            x = torch.tensor(xs, dtype=torch.long).view(-1, 133)\n",
    "        \n",
    "            edge_indices, edge_attrs = [], []\n",
    "            for bond in mol.GetBonds():\n",
    "                i = bond.GetBeginAtomIdx()\n",
    "                j = bond.GetEndAtomIdx()\n",
    "        \n",
    "                edge_feature = [EDGE_FEATURES['possible_bonds'].index(bond.GetBondType())] + [\n",
    "                    EDGE_FEATURES['possible_bond_dirs'].index(bond.GetBondDir())]\n",
    "        \n",
    "                edge_indices += [[i, j], [j, i]]\n",
    "                edge_attrs += [edge_feature, edge_feature]\n",
    "        \n",
    "            edge_index = torch.tensor(edge_indices)\n",
    "            edge_index = edge_index.t().to(torch.long).view(2, -1)\n",
    "            edge_attr = torch.tensor(edge_attrs, dtype=torch.long).view(-1, 2)\n",
    "        \n",
    "            if edge_index.numel() > 0:  # Sort indices.\n",
    "                perm = (edge_index[0] * x.size(0) + edge_index[1]).argsort()\n",
    "                edge_index, edge_attr = edge_index[:, perm], edge_attr[perm]\n",
    "\n",
    "            # label -> y\n",
    "            y = torch.tensor([label], dtype=torch.float).view(1, -1)\n",
    "            graph_list.append(Data(x=x, edge_index=edge_index, edge_attr=edge_attr, smiles=smiles, y=y))\n",
    "    \n",
    "    return graph_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9dfbbadd-309c-4b50-b234-af220a14c0b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "graph_list = smiles_to_graph(data_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "35d8b1e4-14bf-4747-b67b-77956ca39ba9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1513, 1513)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(data_list),len(graph_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ce7856a6-4332-433f-a4af-e5a84f0a04ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 1\n",
    "train_dataset, valid_dataset, test_dataset = random_split(graph_list, null_value=0, frac_train=0.8, frac_valid=0.1, frac_test=0.1, seed=seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8bf344ef-dbeb-45e7-8942-b5710a79e2f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def loader_dataset(data_list, batch_size, shuffle=False):\n",
    "    \"\"\"\n",
    "    DataLoader로 변경\n",
    "    \"\"\"\n",
    "    collate = Batch.from_data_list(data_list)\n",
    "    loader = DataLoader(data_list, batch_size=batch_size, collate_fn=collate, shuffle=shuffle)\n",
    "\n",
    "    return loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9128401d-17ae-4c5f-bd50-c26ccefe1596",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 256\n",
    "train_loader = loader_dataset(data_list=train_dataset, batch_size=batch_size, shuffle=False)\n",
    "valid_loader = loader_dataset(data_list=valid_dataset, batch_size=batch_size, shuffle=False) \n",
    "test_loader = loader_dataset(data_list=test_dataset, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3ef0384d-9b36-41f5-80c6-a5c4a43c288a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataBatch(x=[8647, 133], edge_index=[2, 18706], edge_attr=[18706, 2], y=[256, 1], smiles=[256], batch=[8647], ptr=[257])\n"
     ]
    }
   ],
   "source": [
    "# loader 확인\n",
    "for batch in train_loader:\n",
    "    print(batch)\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba29094d-cdde-4599-97df-e3d27260d78f",
   "metadata": {},
   "source": [
    "### 문자열 token으로 변환"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "96f87735-a14b-4672-aca7-2d24b967b569",
   "metadata": {},
   "outputs": [],
   "source": [
    "def smiles_to_token(data_list):\n",
    "    # token 종류 수집\n",
    "    vocab = []\n",
    "    max_len = 0\n",
    "    tokenizer = BasicSmilesTokenizer()\n",
    "    for data in data_list:\n",
    "        tokens = tokenizer.tokenize(data['smiles'])\n",
    "        max_len = max(max_len, len(tokens))\n",
    "        vocab += tokens\n",
    "        \n",
    "    uniq_vocab = sorted(set(vocab))\n",
    "    smiles_vocab = {v: i for i, v in enumerate(uniq_vocab)}\n",
    "    smiles_vocab['Unk'] = len(smiles_vocab)\n",
    "\n",
    "    # token으로 변환\n",
    "    tokens_list = []\n",
    "    for data in data_list :\n",
    "        label = data['label']\n",
    "        tokens = [smiles_vocab[token] for token in tokenizer.tokenize(data['smiles'])]\n",
    "        pad_len = max_len -len(tokens)\n",
    "        tokens = tokens + ([0]*pad_len) \n",
    "        \n",
    "        x = torch.tensor(tokens, dtype=torch.float).unsqueeze(1)\n",
    "        y = torch.tensor([label], dtype=torch.float).view(1, -1)\n",
    "        tokens_list.append(Data(x=x ,y=y))\n",
    "        \n",
    "    return tokens_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "880d072d-a270-47fb-b025-44bc53d31fbf",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "length of each tokens: 178\n",
      "Data(x=[178, 1], y=[1, 1])\n"
     ]
    }
   ],
   "source": [
    "tokens_list = smiles_to_token(data_list)\n",
    "for tokens in tokens_list:\n",
    "    print(f\"length of each tokens: {len(tokens.x)}\")\n",
    "    print(tokens)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "5caf561e-9d80-426a-9a70-fc929437fcc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 1\n",
    "train_dataset, valid_dataset, test_dataset = random_split(tokens_list, null_value=0, frac_train=0.8, frac_valid=0.1, frac_test=0.1, seed=seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "9c6f5e4a-72f8-4325-990d-cf687042da82",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 256\n",
    "train_loader = loader_dataset(data_list=train_dataset, batch_size=batch_size, shuffle=False)\n",
    "valid_loader = loader_dataset(data_list=valid_dataset, batch_size=batch_size, shuffle=False) \n",
    "test_loader = loader_dataset(data_list=test_dataset, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "48601bd2-bf59-425b-8cff-a3d0eabbd116",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataBatch(x=[45568, 1], y=[256, 1], batch=[45568], ptr=[257])\n"
     ]
    }
   ],
   "source": [
    "# loader 확인\n",
    "for batch in train_loader:\n",
    "    print(batch)\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19c6f714-dc8c-4ba6-97d3-a4a3849d8bc2",
   "metadata": {},
   "source": [
    "### Fingerprint로 변환"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "206d94e2-8417-4126-a157-73e64265e44a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# choice : 'rdkit', 'maccs', 'morgan' 중 변환할  fingerprint 선택\n",
    "def smiles_to_fingerprint(data_list, choice):\n",
    "    fp_list = []\n",
    "    for data in data_list:\n",
    "        smiles = data['smiles']\n",
    "        label = data['label']\n",
    "        \n",
    "        molecule = Chem.MolFromSmiles(smiles)\n",
    "        \n",
    "        if molecule is None :\n",
    "            continue\n",
    "        else:     \n",
    "            if choice == 'rdkit':\n",
    "                rdkit_fp = Chem.RDKFingerprint(molecule)\n",
    "                x = rdkit_fp\n",
    "            elif choice == 'maccs':\n",
    "                maccs_fp = MACCSkeys.GenMACCSKeys(molecule)\n",
    "                x = maccs_fp\n",
    "            elif choice == 'morgan':\n",
    "                morgan_fp = AllChem.GetMorganFingerpirntAsBitVect(molecule, radius=2, nBits=1024)\n",
    "                x = morgan_fp\n",
    "                \n",
    "            x = torch.tensor(x, dtype=torch.float).unsqueeze(1)\n",
    "            y = torch.tensor([label], dtype=torch.float).view(1, -1)\n",
    "            fp_list.append(Data(x=x, y=y))\n",
    "                           \n",
    "    return fp_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "b2206618-ecf1-410a-a90e-c9fa08fce484",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 데이터 불러오기\n",
    "fp_list = smiles_to_fingerprint(data_list, 'rdkit')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "638030d6-0711-46fd-8b80-3a7c36a87652",
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 1\n",
    "train_dataset, valid_dataset, test_dataset = random_split(fp_list, null_value=0, frac_train=0.8, frac_valid=0.1, frac_test=0.1, seed=seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "a14ef7d9-af65-437a-9b88-0c7ef720484d",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 256\n",
    "train_loader = loader_dataset(data_list=train_dataset, batch_size=batch_size, shuffle=False)\n",
    "valid_loader = loader_dataset(data_list=valid_dataset, batch_size=batch_size, shuffle=False) \n",
    "test_loader = loader_dataset(data_list=test_dataset, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "0dc5de8c-981a-41d2-bcb6-6dfdd1489b67",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataBatch(x=[524288, 1], y=[256, 1], batch=[524288], ptr=[257])\n"
     ]
    }
   ],
   "source": [
    "# loader 확인\n",
    "for batch in train_loader:\n",
    "    print(batch)\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95367093-0182-4605-a254-d9fc721c99b9",
   "metadata": {},
   "source": [
    "### Descriptors로 변환"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "25570a5e-96b2-442f-ad22-b8bd2b16af15",
   "metadata": {},
   "outputs": [],
   "source": [
    "def smiles_to_descriptors(data_list):\n",
    "\n",
    "    des_list = []\n",
    "    for data in data_list:\n",
    "        smiles = data['smiles']\n",
    "        label = data['label']\n",
    "        \n",
    "        molecule = Chem.MolFromSmiles(smiles)\n",
    "\n",
    "        # filtering\n",
    "        if molecule is None :\n",
    "            continue\n",
    "        else:\n",
    "            descriptors_dict= Descriptors.CalcMolDescriptors(molecule)\n",
    "            descriptor_vec = np.array([value for value in descriptors_dict.values()]) # dictionary의 value만 추출하여 vector 생성\n",
    "            x = torch.tensor(descriptor_vec, dtype=torch.float).unsqueeze(1)\n",
    "            y = torch.tensor([label], dtype=torch.float).view(1, -1)\n",
    "            des_list.append(Data(x=x, y=y))\n",
    "        \n",
    "    return des_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "1534d57e-303c-4dfa-8bae-cd6edd53118a",
   "metadata": {},
   "outputs": [],
   "source": [
    "des_list = smiles_to_descriptors(data_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "23bd038e-1a7f-42e3-9d56-cdc56cf2ede4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Data(x=[210, 1], y=[1, 1])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "des_list[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "f59df6af-f738-453a-9c42-da91faa18c5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 1\n",
    "train_dataset, valid_dataset, test_dataset = random_split(des_list, null_value=0, frac_train=0.8, frac_valid=0.1, frac_test=0.1, seed=seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "9d94b66f-7aa8-4c23-8a64-99bb8aa2392d",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 256\n",
    "train_loader = loader_dataset(data_list=train_dataset, batch_size=batch_size, shuffle=False)\n",
    "valid_loader = loader_dataset(data_list=valid_dataset, batch_size=batch_size, shuffle=False) \n",
    "test_loader = loader_dataset(data_list=test_dataset, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "fae043f0-69cf-4565-b90f-c8f883e1db80",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataBatch(x=[53760, 1], y=[256, 1], batch=[53760], ptr=[257])\n"
     ]
    }
   ],
   "source": [
    "for batch in train_loader:\n",
    "    print(batch)\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "beb5b904-0bfd-477f-9d0a-f5f68a64f3bd",
   "metadata": {},
   "source": [
    "### 3D Graph로 변환"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "3e01e741-5616-4e1e-8061-5ff1d9151eb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def molecule_to_3d(data_list):\n",
    "    \n",
    "    graph3d_list = []\n",
    "    for data in data_list:\n",
    "        smiles = data['smiles']\n",
    "        label = data['label']\n",
    "        \n",
    "        molecule = Chem.MolFromSmiles(smiles)\n",
    "\n",
    "        # filtering\n",
    "        if molecule is None :\n",
    "            continue\n",
    "        else:              \n",
    "            atom_info = [(atom.GetIdx(), atom.GetSymbol()) for atom in molecule.GetAtoms()]             \n",
    "            status = rdDistGeom.EmbedMolecule(molecule)\n",
    "            \n",
    "            # 3d graph 변환 filtering\n",
    "            if status != 0: # 0이 아닌 경우 변환 실패\n",
    "                continue\n",
    "            else:\n",
    "                conf = molecule.GetConformer()\n",
    "                pos = np.array([conf.GetAtomPosition(idx) for idx, symbol in atom_info])\n",
    "            \n",
    "                graph_data = pyg_utils.from_smiles(smiles)\n",
    "                graph_data.pos = pos\n",
    "                graph_data.y = torch.tensor([label], dtype=torch.float).view(1, -1)\n",
    "                graph3d_list.append(graph_data)\n",
    "        \n",
    "    return graph3d_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "cc428e45-f661-4635-9023-c42a6ff15fb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 데이터 불러오기\n",
    "graph3d_list = molecule_to_3d(data_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "7d5b9462-93c5-4226-a742-fd8bca992f2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 1\n",
    "train_dataset, valid_dataset, test_dataset = random_split(graph3d_list, null_value=0, frac_train=0.8, frac_valid=0.1, frac_test=0.1, seed=seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "daa7a078-09f8-4d6b-a8f4-5a4eb2ea4635",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 256\n",
    "train_loader = loader_dataset(data_list=train_dataset, batch_size=batch_size, shuffle=False)\n",
    "valid_loader = loader_dataset(data_list=valid_dataset, batch_size=batch_size, shuffle=False) \n",
    "test_loader = loader_dataset(data_list=test_dataset, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "173cabc0-58ed-457d-9780-b7ab836418c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataBatch(x=[8588, 9], edge_index=[2, 18586], edge_attr=[18586, 3], smiles=[256], pos=[256], y=[256, 1], batch=[8588], ptr=[257])\n"
     ]
    }
   ],
   "source": [
    "for batch in train_loader:\n",
    "    print(batch)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46311298-9764-4ced-b686-63ae904a9a81",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
